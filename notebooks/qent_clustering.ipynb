{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to cluster query results\n",
    "\n",
    "1. Convert table outputs (including column names into html like strings)\n",
    "2. Use MarkupLM to generate feature vectors from these html strings\n",
    "3. use DBSCAN to cluster the strings together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\varun\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\sqlcopilot-cwclWRYE-py3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('./..')\n",
    "import json\n",
    "from collections import Counter\n",
    "from transformers import AutoProcessor, MarkupLMModel\n",
    "import sqlite3\n",
    "import torch\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# loading in dev set\n",
    "with open('./data/grid_search_subsampled_dev_set.json', 'r') as f:\n",
    "    dev_set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert any sql query output - column_names: list[str] and result: list[tuple[str]] into a html string\n",
    "def sql_result_to_html(\n",
    "    column_names: list[str],\n",
    "    result: list[tuple[str]],\n",
    "    error: str = None\n",
    "):\n",
    "\n",
    "    # Generate HTML content\n",
    "    html_content = \"\"\"\n",
    "<body>\n",
    "    <table>\n",
    "        <thead>\n",
    "            <tr>\n",
    "\"\"\"\n",
    "\n",
    "    if error:\n",
    "        html_content += f\"                <th>{error}</th>\\n\"\n",
    "        html_content += \"\"\"\n",
    "    </tr>\n",
    "</thead>\"\"\"\n",
    "\n",
    "    else:    \n",
    "        # Add headers to the table\n",
    "        for header in column_names:\n",
    "            html_content += f\"                <th>{header}</th>\\n\"\n",
    "\n",
    "        html_content += \"\"\"\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "\"\"\"\n",
    "\n",
    "        # Add rows for the data\n",
    "        for row in result:\n",
    "            html_content += \"            <tr>\\n\"\n",
    "            for cell in row:\n",
    "                html_content += f\"                <td>{cell}</td>\\n\"\n",
    "            html_content += \"            </tr>\\n\"\n",
    "\n",
    "        # Close the HTML table and body\n",
    "        html_content += \"\"\"\n",
    "        </tbody>\n",
    "    </table>\n",
    "</body>\n",
    "\"\"\"\n",
    "    return html_content\n",
    "\n",
    "def html_to_features(\n",
    "    html_string: str,\n",
    "    markup_lm_processor: AutoProcessor,\n",
    "    markup_lm_model: MarkupLMModel\n",
    "):\n",
    "    # Have to compromise with truncation, max context length of markup lm is 512\n",
    "    encoding = markup_lm_processor(html_string, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    outputs = markup_lm_model(**encoding)\n",
    "    last_hidden_states = outputs.last_hidden_state.mean(dim=1)\n",
    "    return last_hidden_states\n",
    "\n",
    "def chunk_html(html_string: str, max_tokens: int, processor: AutoProcessor):\n",
    "    tokens = processor(html_string, return_tensors=\"pt\", truncation=False, padding=False)[\"input_ids\"][0]\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunks.append(html_string[i:i + max_tokens])\n",
    "    return chunks\n",
    "\n",
    "def chunked_html_to_features(\n",
    "    html_string: str,\n",
    "    markup_lm_processor: AutoProcessor,\n",
    "    markup_lm_model: MarkupLMModel,\n",
    "    max_tokens: int = 512\n",
    "):\n",
    "    chunks = chunk_html(html_string, max_tokens, markup_lm_processor)\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        # Have to compromise, max context length of markup lm is 512\n",
    "        encoding = markup_lm_processor(chunk, return_tensors=\"pt\", truncation=True, max_tokens=512)\n",
    "        outputs = markup_lm_model(**encoding)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1))\n",
    "    return torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\varun\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\sqlcopilot-cwclWRYE-py3.10\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: no such column: f.District\n",
      "An error occurred: no such column: frpm.District\n",
      "An error occurred: no such column: District\n",
      "An error occurred: no such column: f.District\n",
      "An error occurred: no such column: f.District\n",
      "An error occurred: no such column: District\n",
      "An error occurred: near \"SELECT\": syntax error\n",
      "An error occurred: no such column: f.District\n",
      "An error occurred: no such column: District\n",
      "An error occurred: no such column: District\n",
      "An error occurred: no such column: f.District\n",
      "An error occurred: no such column: District\n",
      "An error occurred: no such column: District\n",
      "An error occurred: no such column: f.District\n",
      "An error occurred: no such column: District\n",
      "An error occurred: no such column: f.District\n",
      "An error occurred: near \"SELECT\": syntax error\n",
      "An error occurred: no such column: f.District\n",
      "An error occurred: no such column: District\n",
      "An error occurred: no such column: f.District\n",
      "An error occurred: no such column: District\n",
      "An error occurred: no such column: f.District\n",
      "An error occurred: no such column: District\n",
      "An error occurred: near \"SELECT\": syntax error\n",
      "An error occurred: no such column: T1.District\n",
      "An error occurred: no such column: T1.District\n",
      "An error occurred: near \"Code\": syntax error\n",
      "An error occurred: no such column: T1.District\n",
      "An error occurred: no such column: T1.District\n",
      "An error occurred: no such column: T1.District\n",
      "An error occurred: no such column: T1.District\n",
      "An error occurred: no such column: T1.District\n",
      "An error occurred: near \"Code\": syntax error\n",
      "An error occurred: no such column: T1.District\n",
      "An error occurred: no such column: T1.District\n",
      "An error occurred: no such column: T1.District\n",
      "An error occurred: no such column: T1.District\n",
      "An error occurred: near \"Code\": syntax error\n",
      "An error occurred: near \"Code\": syntax error\n",
      "An error occurred: no such column: T1.District\n",
      "An error occurred: near \"Code\": syntax error\n",
      "An error occurred: no such column: T1.District\n",
      "An error occurred: no such column: T1.District\n",
      "An error occurred: near \"Code\": syntax error\n",
      "An error occurred: no such column: T1.District\n",
      "An error occurred: near \"Code\": syntax error\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./data/test.json', 'r') as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "queries = [(q, \"california_schools\") for q in (queries['equivalent_queries'] + queries['different_queries'])]\n",
    "\n",
    "markup_processor = AutoProcessor.from_pretrained(\"microsoft/markuplm-base\")\n",
    "markup_model = MarkupLMModel.from_pretrained(\"microsoft/markuplm-base\")\n",
    "\n",
    "# executing the result of these queries and gathering them in a list\n",
    "# for unsuccessful queries, we cluster them based on the semantic information present in the error messages\n",
    "all_features = []\n",
    "for query in queries:\n",
    "    sql_query = query[0]\n",
    "    db_id = query[1]\n",
    "    try:\n",
    "        # Connect to the SQLite database\n",
    "        conn = sqlite3.connect(f\"{os.environ['DB_ROOT_DIRECTORY']}/{db_id}/{db_id}.sqlite\")\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Execute the query\n",
    "        cursor.execute(sql_query)\n",
    "        \n",
    "        # Fetch all results\n",
    "        results = cursor.fetchall()\n",
    "        columns = [description[0] for description in cursor.description]\n",
    "        html_result = sql_result_to_html(column_names=columns, result=results)\n",
    "        # print(html_result)\n",
    "        features = html_to_features(\n",
    "            html_string=html_result, \n",
    "            markup_lm_processor=markup_processor, \n",
    "            markup_lm_model=markup_model,\n",
    "        )\n",
    "        all_features.append(features.detach().squeeze(dim=0))\n",
    "        \n",
    "            \n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        html_result = sql_result_to_html(error=e)\n",
    "        features = html_to_features(\n",
    "            html_string=html_result, \n",
    "            markup_lm_processor=markup_processor, \n",
    "            markup_lm_model=markup_model,\n",
    "        )\n",
    "        all_features.append(features.detach().squeeze(dim=0))\n",
    "\n",
    "    finally:\n",
    "        # Close the connection\n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(all_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mall_features\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_features' is not defined"
     ]
    }
   ],
   "source": [
    "print(all_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering all_features\n",
    "def cluster_sql_queries(embeddings: np.ndarray):\n",
    "    \"\"\"\n",
    "    Cluster SQL query embeddings using DBSCAN.\n",
    "    \"\"\"\n",
    "    # Normalize the embeddings\n",
    "    scaler = StandardScaler()\n",
    "    normalized_embeddings = scaler.fit_transform(embeddings)\n",
    "    \n",
    "    # Apply DBSCAN\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=1)  # You may need to tune these parameters\n",
    "    clusters = dbscan.fit_predict(normalized_embeddings)\n",
    "    \n",
    "    return clusters.tolist()\n",
    "\n",
    "# test\n",
    "clusters_DB = cluster_sql_queries(embeddings=np.array(all_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of semantic entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate semantic entropy for clusters\n",
    "def calculate_semantic_entropy(clusters: list[int]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the semantic entropy of the clusters using the formula: \n",
    "    -Sigma(Pi * log(Pi)), where Pi is the number of candidates in cluster i divided by all candidates.\n",
    "    \"\"\"\n",
    "    cluster_counts = Counter(clusters)\n",
    "    entropy = 0\n",
    "    for cluster_id, count in cluster_counts.items():\n",
    "        Pi = count / len(clusters)\n",
    "        entropy -= Pi * np.log2(Pi)\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(calculate_semantic_entropy(clusters=clusters_DB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maximum possible value of entropy\n",
    "np.log2(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1:\n",
      "  Method a: 0.33%\n",
      "  Method b: 0.33%\n",
      "  Method c: 0.33%\n",
      "Cluster 2:\n",
      "  Method c: 0.50%\n",
      "  Method a: 0.50%\n",
      "Cluster 3:\n",
      "  Method a: 0.33%\n",
      "  Method b: 0.67%\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Sample data\n",
    "clusters = [1, 1, 3, 2, 3, 2, 1, 3]\n",
    "methods = ['a', 'b', 'a', 'c', 'b', 'a', 'c', 'b']\n",
    "\n",
    "# Create a dictionary to store method counts for each cluster\n",
    "cluster_method_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Populate the method counts for each cluster\n",
    "for cluster, method in zip(clusters, methods):\n",
    "    cluster_method_counts[cluster][method] += 1\n",
    "\n",
    "# Compute percentages for each method in each cluster\n",
    "cluster_percentages = {}\n",
    "for cluster, method_counts in cluster_method_counts.items():\n",
    "    total_count = sum(method_counts.values())\n",
    "    cluster_percentages[cluster] = {\n",
    "        method: (count / total_count) for method, count in method_counts.items()\n",
    "    }\n",
    "\n",
    "# Display the results\n",
    "for cluster, percentages in sorted(cluster_percentages.items()):\n",
    "    print(f\"Cluster {cluster}:\")\n",
    "    for method, percentage in percentages.items():\n",
    "        print(f\"  Method {method}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'a': 0.3333333333333333,\n",
       "  'b': 0.3333333333333333,\n",
       "  'c': 0.3333333333333333},\n",
       " 3: {'a': 0.3333333333333333, 'b': 0.6666666666666666},\n",
       " 2: {'c': 0.5, 'a': 0.5}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqlcopilot-cwclWRYE-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
